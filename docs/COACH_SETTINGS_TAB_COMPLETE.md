# Evolution Coach Settings Tab - Complete Implementation

## Overview

Added a new **"Evolution Coach"** tab to the Settings dialog where all coach-related parameters can be configured and saved to `.env`.

## Settings Tab Content

### Evolution Coach Settings Group

1. **First Analysis At** - Generation number when first coach analysis triggers
   - Range: 1-1000
   - Default: 10 gen
   - Setting: `COACH_FIRST_ANALYSIS_GENERATION`

2. **Max Log Generations** - Keep last N generations in log history sent to coach
   - Range: 5-100
   - Default: 25 gens
   - Setting: `COACH_MAX_LOG_GENERATIONS`

3. **Auto Reload** - Checkbox: Auto reload model after recommendations
   - Default: Checked (true)
   - Setting: `COACH_AUTO_RELOAD_MODEL`

4. **Context Length** - Model context window size
   - Range: 1,000-131,072 tokens
   - Default: 5,000 tokens
   - Setting: `COACH_CONTEXT_LENGTH`
   - Note: 131,072 = Gemma's maximum

5. **GPU Offload** - GPU offload ratio
   - Range: 0.0-1.0
   - Default: 0.6 (60%)
   - Setting: `COACH_GPU`
   - 0.0 = CPU only, 1.0 = max GPU

### CPU Worker Settings Group

6. **CPU Workers** - Worker processes for CPU-based operations
   - Range: 1 to CPU count
   - Default: 7 workers
   - Setting: `CPU_WORKERS`

7. **ADAM Epsilon Stability** - ADAM optimizer epsilon stability parameter
   - Range: 1e-10 to 1e-6
   - Default: 1e-8
   - Setting: `ADAM_EPSILON_STABILITY`

### Buttons

- **Reset Coach Defaults** - Resets all values to defaults
- **ðŸ’¾ Save Settings** - Saves all settings to `.env`

## Persistence

All settings are:
1. **Loaded** from `.env` when Settings dialog opens
2. **Saved** to `.env` when "Save Settings" clicked
3. **Reloaded** globally via `SettingsManager.reload_settings()`

## Save Confirmation

After saving, you'll see:
```
âœ“ Parameters saved to .env
Settings Saved

All settings have been saved successfully!

Your configuration will be restored the next time you open the application.
```

## Files Modified

### UI Layer
- `app/widgets/settings_dialog.py`
  - Added `create_coach_tab()` method
  - Added `reset_coach_params()` method
  - Updated `load_from_settings()` to load coach settings
  - Updated `save_settings()` to save coach settings

### Configuration Layer
- `config/settings.py`
  - Changed `coach_context_length: int = 5000` (was 10000)
  - Added `cpu_workers: int = 7`

- `.env.example`
  - `COACH_CONTEXT_LENGTH=5000` (was 10000)
  - `CPU_WORKERS=7` (already existed)

### Coach Client
- `backtest/llm_coach.py`
  - Changed default `context_length=5000` (was 10000)

## Usage Workflow

### 1. Open Settings
Menu â†’ Settings (or press shortcut)

### 2. Navigate to Evolution Coach Tab
Click "Evolution Coach" tab (last tab)

### 3. Adjust Parameters
- Change first analysis generation if desired
- Adjust context length based on your needs
  - 5000 tokens: Fast, suitable for 25 generations
  - 10000 tokens: More headroom
  - 131072 tokens: Maximum (slower, more memory)
- Set GPU offload based on your hardware
  - 0.6 = Good balance
  - 1.0 = Max GPU (if you have lots of VRAM)
  - 0.3 = More CPU/less GPU

### 4. Save Settings
Click "ðŸ’¾ Save Settings"

### 5. Settings Applied
- All parameters immediately saved to `.env`
- Global settings reloaded
- Coach will use new settings on next load

## Context Length Tuning Guide

### Current Default: 5000 tokens

**Typical Token Usage (25 generations):**
- System prompt: ~500 tokens
- Evolution state: ~300 tokens
- 25 generations logs: ~2,000-3,000 tokens
- Parameters config: ~200 tokens
- **Total input**: ~3,000-4,000 tokens
- **Response**: ~500-1,000 tokens
- **Grand total**: ~4,000-5,000 tokens

**Result**: 5,000 is perfect for 25 generations! âœ…

### When to Increase

**Increase to 10,000 if:**
- You want to keep more than 25 generations (e.g., 50 gens)
- Logs are very verbose
- You see truncation warnings

**Increase to 131,072 (max) if:**
- You want full GA run history (100+ generations)
- You have powerful GPU and want maximum context
- Speed is not a concern

### Performance Impact

| Context Length | Load Time | Inference Speed | Memory Usage |
|----------------|-----------|-----------------|--------------|
| 5,000 tokens   | Fast      | Fast            | Low          |
| 10,000 tokens  | Fast      | Medium          | Medium       |
| 50,000 tokens  | Slow      | Slow            | High         |
| 131,072 tokens | Very Slow | Very Slow       | Very High    |

**Recommendation**: Start with 5,000, increase only if needed.

## Monitoring Token Usage

The Evolution Coach window shows estimated token usage:
```
[COACH  ] Sending evolution state:
[COACH  ]   - Log lines: 87
[COACH  ]   - Estimated input tokens: ~2,345
[COACH  ] ðŸ“¥ Received 1234 chars from LLM
[COACH  ]   - Tokens: 456
```

Watch these logs to verify if your context length is sufficient!

## Default Configuration

The default configuration (5,000 context, 60% GPU, 25 gens) is optimized for:
- Fast inference (< 5s response time)
- Low memory usage (~3-4GB VRAM)
- Sufficient context for typical GA runs
- Good balance between speed and capability

## Advanced Configuration

### For Powerful Hardware (RTX 4090, etc.)
```
COACH_CONTEXT_LENGTH=131072
COACH_GPU=1.0
COACH_MAX_LOG_GENERATIONS=100
```

### For Limited Hardware (Laptop, etc.)
```
COACH_CONTEXT_LENGTH=5000
COACH_GPU=0.3
COACH_MAX_LOG_GENERATIONS=15
CPU_WORKERS=4
```

### For Maximum Speed (Minimal Context)
```
COACH_CONTEXT_LENGTH=3000
COACH_GPU=0.6
COACH_MAX_LOG_GENERATIONS=10
```

## Troubleshooting

### Settings Not Persisting
- Make sure you clicked "ðŸ’¾ Save Settings"
- Check that `.env` file exists in project root
- Verify settings with: `cat .env | grep COACH_`

### Context Too Small
- Increase `COACH_CONTEXT_LENGTH` to 10,000
- Or reduce `COACH_MAX_LOG_GENERATIONS` to 15
- Check estimated tokens in Evolution Coach window

### Model Loading Slow
- Reduce `COACH_CONTEXT_LENGTH` to 3,000
- Or reduce `COACH_GPU` to 0.4 (more CPU offload)

### Out of Memory
- Reduce `COACH_CONTEXT_LENGTH` to 5,000
- Reduce `COACH_GPU` to 0.4
- Reduce `COACH_MAX_LOG_GENERATIONS` to 15

## Testing

### Verify Settings Save
1. Open Settings â†’ Evolution Coach
2. Change "First Analysis At" to 20
3. Change "Context Length" to 8000
4. Click "Save Settings"
5. Close and reopen Settings
6. Verify values are 20 and 8000 âœ“

### Verify Settings Used
1. Load model with settings
2. Check Evolution Coach window logs:
   ```
   [LMS    ] ðŸ“¦ Loading model: google/gemma-3-12b
   [LMS    ]   - GPU offload: 60.0%
   [LMS    ]   - Context length: 5000
   ```

### Verify Token Estimation
1. Run GA for 30 generations
2. Check Evolution Coach window at gen 10:
   ```
   [COACH  ]   - Estimated input tokens: ~3,456
   ```
3. Verify < 5000 (fits in context!) âœ“

## Summary

âœ… Complete Evolution Coach settings tab added
âœ… All 7 parameters configurable in UI
âœ… Settings persist to `.env` file
âœ… Settings load on startup
âœ… Default context length: 5000 tokens (optimized for 25 gens)
âœ… Token usage monitoring in Evolution Coach window
âœ… Reset defaults button included
âœ… Comprehensive tooltips and info panel

The Evolution Coach is now fully configurable via the Settings dialog!
